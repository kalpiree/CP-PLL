class CIFAR100_Augmentention(Dataset):
    def __init__(self, images, given_label_matrix, true_labels):
        self.images = images
        self.given_label_matrix = given_label_matrix
        self.true_labels = true_labels
        self.weak_transform = transforms.Compose(
            [
                transforms.ToPILImage(),
                transforms.RandomResizedCrop(size=32, scale=(0.2, 1.)),
                transforms.RandomHorizontalFlip(),
                transforms.RandomApply([
                    transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)
                ], p=0.8),
                transforms.RandomGrayscale(p=0.2),
                transforms.ToTensor(),
                transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])
        self.strong_transform = transforms.Compose(
            [
                transforms.ToPILImage(),
                transforms.RandomResizedCrop(size=32, scale=(0.2, 1.)),
                transforms.RandomHorizontalFlip(),
                RandomAugment(3, 5),
                transforms.ToTensor(),
                transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])

    def __len__(self):
        return len(self.true_labels)

    def __getitem__(self, index):
        each_image_w = self.weak_transform(self.images[index])
        each_image_s = self.strong_transform(self.images[index])
        each_label = self.given_label_matrix[index]
        each_true_label = self.true_labels[index]
        return each_image_w, each_image_s, each_label, each_true_label, index   ########cifar100



def gen_imbalanced_data(self, data, targets, num_classes, imb_type='exp', imb_factor=0.01):
        img_max = len(data) / num_classes
        img_num_per_cls = self.get_img_num_per_cls(num_classes, imb_type, imb_factor, img_max)

        new_data = []
        new_targets = []
        targets_np = np.array(targets, dtype=np.int64)
        classes = np.unique(targets_np)

        for the_class, the_img_num in zip(classes, img_num_per_cls):
            idx = np.where(targets_np == the_class)[0]
            np.random.shuffle(idx)
            selec_idx = idx[:the_img_num]
            new_data.append(data[selec_idx, ...])
            new_targets.extend([the_class] * the_img_num)

        new_data = np.vstack(new_data)
        new_targets = np.array(new_targets)

        return new_data, new_targets


def load_cifar100(partial_rate, batch_size, hierarchical):
    test_transform = transforms.Compose(
        [transforms.ToTensor(),
         transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])

    temp_train = cifar100(root='./cifar100_lt/',
                          download=True,
                          train_or_not=True,
                          val_or_not= False,
                          transform=transforms.Compose([transforms.ToTensor(),
                                                        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)), ]),
                          imb_factor=0.1,
                          imbalance=True
                          )
    val_dataset = cifar100(root='./cifar100_lt/',
                           download=True,
                           train_or_not=False,
                           val_or_not= True,
                           transform=transforms.Compose([transforms.ToTensor(),
                                                         transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)), ]),
                           imb_factor=0.1,
                           imbalance=True
                           )
    test_dataset = cifar100(root='./cifar100_lt/',
                            download=True,
                            train_or_not=False,
                            val_or_not= False,
                            transform=transforms.Compose([transforms.ToTensor(),
                                                          transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)), ]),
                            # partial_type=args.partial_type,
                            # partial_rate=args.partial_rate,
                            imb_factor=0.1,
                            imbalance=True
                            )

    # temp_train = cifar100(root='./data', train=True, download=True)
    # data, labels = temp_train.data, torch.Tensor(temp_train.targets).long()
    # get original data and labels
    data = temp_train.train_data
    #labels = torch.Tensor(temp_train.train_labels).long()
    labels = torch.Tensor(temp_train.train_labels).float()

    # test_dataset = cifar100(root='./data', train=False, transform=test_transform) test_loader =
    # torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size*4, shuffle=False, num_workers=4,
    # sampler=torch.utils.data.distributed.DistributedSampler(test_dataset, shuffle=False))

    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size * 4, shuffle=False,
                                             num_workers=4,
                                             sampler=torch.utils.data.distributed.DistributedSampler(val_dataset,
                                                                                                     shuffle=False))
    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size * 4, shuffle=False,
                                              num_workers=4,
                                              sampler=torch.utils.data.distributed.DistributedSampler(test_dataset,
                                                                                                      shuffle=False))

    if hierarchical:
        partialY = generate_hierarchical_cv_candidate_labels('cifar100', labels, partial_rate)
        # for fine-grained classification
    else:
        partialY = generate_uniform_cv_candidate_labels(labels, partial_rate)

    temp = torch.zeros(partialY.shape)
    #temp[torch.arange(partialY.shape[0]), labels] = 1
    temp[torch.arange(partialY.shape[0]), labels.long()] = 1
    if torch.sum(partialY * temp) == partialY.shape[0]:
        print('partialY correctly loaded')
    else:
        print('inconsistent permutation')
    print('Average candidate num: ', partialY.sum(1).mean())
    partial_matrix_dataset = CIFAR100_Augmentention(data, partialY.float(), labels.float())
    train_sampler = torch.utils.data.distributed.DistributedSampler(partial_matrix_dataset)
    partial_matrix_train_loader = torch.utils.data.DataLoader(dataset=partial_matrix_dataset,
                                                              batch_size=batch_size,
                                                              shuffle=(train_sampler is None),
                                                              num_workers=4,
                                                              pin_memory=True,
                                                              sampler=train_sampler,
                                                              drop_last=True)
    return partial_matrix_train_loader, partialY, train_sampler, test_loader, val_loader

